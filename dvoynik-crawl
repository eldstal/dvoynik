#!/bin/env python3

import argparse
from multiprocessing import Queue, Process
import queue
import sys
import time
import random
import traceback
import os
import json
import re

import requests
import tqdm
from playwright.sync_api import sync_playwright, TimeoutError

from util import domains_from_zonefile, lines_from_textfile, prefix_for_hostname, filename_for_url


parser = argparse.ArgumentParser("Scan and screenshot a list of websites")
parser.add_argument("-z", "--zonefile", type=str, default=None, help="DNS Zone file to read domains from")
parser.add_argument("-d", "--domains", type=str, default=None, help="Text file of domains to query")
parser.add_argument("-w", "--workdir", type=str, default=None, required=True, help="Work directory where screenshots are saved")
parser.add_argument("-c", "--clobber", action="store_true", help="Overwrite existing screenshots")

parser.add_argument("-n", "--number-of-workers", type=int, default=1, help="Number of parallel fetches")

conf = parser.parse_args()

meta_db_path = os.path.join(conf.workdir, "metadata.json")
screenshot_dir = os.path.join(conf.workdir, "screenshots")


def save_meta_database(hashdb):
    json.dump(hashdb, open(meta_db_path, "w+"))

def load_meta_database():
    try:
        db = json.load(open(meta_db_path, "r"))
        return db
    except:
        return { "metadata": {} }

def quick_test_url(url):
    try:
        res = requests.get(url, verify=False, timeout=0.5, allow_redirects=True)
        if res.status_code != 200: return False
    except:
        # Not a web host. No need to launch chrome.
        return False

    return True

def fetch_page_metadata(url, metadata):
    metadata["url"] = url

    res = requests.get(url, verify=False, timeout=1, allow_redirects=True)

    re_title = re.compile("<title>(?P<title>[^<]*)<\/title>", re.M | re.I)
    m = re_title.search(res.text)
    if m:
        metadata["title"] = m.group("title").strip()

    return metadata

def snapshot_target(conf, target, metadata, browser):

    url_options = []

    if target.startswith("https://") or target.startswith("http://"):
        url_options  = [ target ]
    else:
        url_options = [
            f"https://{target}",
            f"https://www.{target}",
            f"http://{target}",
            f"http://www.{target}",
        ]

    filepaths = [ filename_for_url(u) for u in url_options ]
    filepaths = [ os.path.join(screenshot_dir, p) for p in filepaths ]

    if None in filepaths:
        return (target, None, 4, "Invalid URL")

    url = None
    for option in url_options:
        if quick_test_url(option):
            url = option
            break

    if not url:
        # Not a web host. No need to launch chrome.
        return (target, None, 2, "No web service")

    url = url_options[0]
    filename = filename_for_url(url)
    filepath = os.path.join(screenshot_dir, filename)

    screenshot_already_exists = False
    for fp in filepaths:
        if os.path.exists(fp):
            screenshot_already_exists = True
            metadata["screenshot"] = os.path.relpath(fp, screenshot_dir)

    # Don't bother screenshotting again
    if not conf.clobber and screenshot_already_exists:
        # Don't overwrite that file. Just move on.
        pass

    else:

        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        # Connect and take a screenshot
        page = browser.new_page()
        page.goto(url)
        page.screenshot(path=filepath)
        page.close()
    
        metadata = fetch_page_metadata(url, metadata)


    return (target, metadata, 1, "OK")


def worker(conf, work_queue, result_queue):

    # Don't spam the terminal because of everyone's messed up SSL configs
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    with sync_playwright() as playwright:
        browser = playwright.chromium.launch()
        context = browser.new_context(ignore_https_errors=True)

        while True:
            job = work_queue.get()

            if job == None:
                break

            target, metadata = job

            try:
                result = snapshot_target(conf, target, metadata, context)
            except TimeoutError:
                result = (target, None, 4, "Timeout")
            except:
                #traceback.print_exc()
                #print("Ignoring that error and moving on to the next site.")
                result = (target, None, 5, "Unhandled Exception")

                context.close()
                browser.close()
                browser = playwright.chromium.launch()
                context = browser.new_context(ignore_https_errors=True)

            result_queue.put(result)

        browser.close()
        #print("Worker process terminating.")

def start_workers(conf, work_queue, result_queue):
    ret = []

    for i in range(conf.number_of_workers):
        w = Process(target=worker, args=(conf, work_queue, result_queue))
        w.daemon = True
        w.start()
        ret.append(w)

    return ret

def stop_workers(conf, workers, work_queue):

    while True:
        try:
            work_queue.get(block=False)
        except queue.Empty:
            break

    for w in workers:
        work_queue.put(None)

    for w in workers:
        w.join()

    print("Terminated all workers.")

    return []


def main():

    targets = [ ]

    metadb = load_meta_database()

    if conf.zonefile:
        targets += domains_from_zonefile(conf.zonefile)

    if conf.domains:
        targets += lines_from_textfile(conf.domains)

    work_queue = Queue(maxsize=len(targets))
    result_queue = Queue(maxsize=len(targets))

    for t in targets:
        metadata = metadb["metadata"].get(t, {})
        work_queue.put((t, metadata))

    workers = start_workers(conf, work_queue, result_queue)


    n_completed = 0
    save_interval = max(500, len(targets) // 100)
    try:
        progress = tqdm.tqdm(range(len(targets)))
        for i in progress:
            target, metadata, severity, status = result_queue.get()
            progress.set_description(f"{target:<30}")
            if severity > 3:
                progress.write(f"{target:<30}: {status}")

            if metadata:
                metadb["metadata"][target] = metadata

                n_completed += 1
                if n_completed % save_interval == 0:
                    save_meta_database(metadb)

    except Exception as e:
        workers = stop_workers(conf, workers, work_queue)

        raise e

    workers = stop_workers(conf, workers, work_queue)

    save_meta_database(metadb)

    print("Exiting.")


if __name__ == "__main__":
    sys.exit(main())
